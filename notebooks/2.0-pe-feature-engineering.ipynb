{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import spacy \n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "import string\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "import unicodedata\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing,model_selection,metrics,feature_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/interim/001-pe-initial-clean.csv',encoding=\"latin-1\",sep=';',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a text processing unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLTKPreprocesor(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    This is a powerful class that can take care of different things.\n",
    "    \"\"\"\n",
    "    def __init__(self,stopwords = None,punct = None,lower = True,strip=True):\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = stopwords or set(sw.words('english'))\n",
    "        self.punct = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self,X):\n",
    "        pass\n",
    "\n",
    "    def transform(self,X):\n",
    "        out = []\n",
    "        for doc in X:\n",
    "            temp = []\n",
    "            for w in self.tokenize(doc):\n",
    "                temp.append(w)\n",
    "            out.append(temp)\n",
    "        return out\n",
    "\n",
    "    def tokenize(self,document):\n",
    "\n",
    "        for sent in sent_tokenize(document):\n",
    "            for token,tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "                token = token.strip('#') if self.strip else token\n",
    "\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                if len(token) <= 0:\n",
    "                    continue\n",
    "\n",
    "                lemma = self.lemmatize(token,tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self,token,tag):\n",
    "        tag ={\n",
    "            'N' : wn.NOUN,\n",
    "            'V' : wn.VERB,\n",
    "            'R' : wn.ADV,\n",
    "            'J' : wn.ADJ\n",
    "        }.get(tag[0],wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token,tag)\n",
    "    \n",
    "# Remove accent chars\n",
    "def remove_accents(x):\n",
    "    return unicodedata.normalize('NFKD',x).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "\n",
    "\n",
    "# Removes special charachters\n",
    "def remove_special_chars(x):\n",
    "    return re.sub('[^a-zA-Z0-9\\s]', '', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping the duplicates again!\n",
    "data.loc[data.category==\"other works on paper\",\"category\"] = \"painting\"\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['artist_name', 'auction_date', 'category', 'hammer_price', 'location','materials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83054, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replacement_neg_hammer_price = data.loc[\n",
    "    np.logical_and(data.hammer_price < 0,np.logical_not(data.estimate_high.isnull()))\n",
    "    ,['estimate_high','estimate_low']].mean(axis=1)\n",
    "\n",
    "data.loc[replacement_neg_hammer_price.index,'hammer_price'] = replacement_neg_hammer_price\n",
    "\n",
    "data = data.loc[data.hammer_price > 0]\n",
    "\n",
    "data = data.drop(['estimate_high','estimate_low'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[np.logical_and(data.materials==\"oil on canvas\",data.category==\"unclassified\"),'category']=\"painting\"\n",
    "\n",
    "data.loc[np.logical_and(data.materials==\"works on paper\",data.category==\"unclassified\"),\"category\"]=\"painting\"\n",
    "\n",
    "data.loc[np.logical_and(data.materials==\"oil and charcoal\",data.category==\"unclassified\"),\"category\"]=\"painting\"\n",
    "\n",
    "data.loc[np.logical_and(data.materials==\"sculpture\",data.category==\"unclassified\"),\"category\"]=\"sculpture\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(data.loc[np.logical_and(data.category==\"unclassified\",data.materials.isnull())].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('materials',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop('title',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_location(x):        \n",
    "    x = str(x).lower()\n",
    "    if \",\" in x:\n",
    "        return x.split(',')[-1].strip(\" \")\n",
    "    return x.strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'location']=data.location.apply(clean_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_locs = list(data.location.value_counts()[data.location.value_counts() > 100].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[~data.location.isin(valid_locs),\"location\"] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[data.location==\"nan\",\"location\"] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'auction_date'] = pd.to_datetime(data.auction_date)\n",
    "data = data.assign(year =[x.year for x in data.auction_date], month=[x.month for x in data.auction_date],day=[x.day for x in data.auction_date],week=[x.week for x in data.auction_date])\n",
    "data = data.drop('auction_date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_features = ['artist_name','category','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data,columns=text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79473, 167)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/interim/002-pe-features.csv\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
